{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Wiki_Hurricane_Scraper import *\n",
    "# from YouTube_Analytics import *\n",
    "from FEMA_Scraper import *\n",
    "from NOAA_Scraper import *\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from BokehPlots import *\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDisasterInfo():\n",
    "    hurricaneDf = getHurricaneData()\n",
    "    hurricaneDf = hurricaneDf[hurricaneDf['Season'] >= 2005].set_index(\"Name\")\n",
    "    name = hurricaneDf.index.tolist()\n",
    "    season = hurricaneDf['Season'].tolist()\n",
    "    locations = getAllLocations()\n",
    "    disastersInfo = list(zip(name, season))\n",
    "    return hurricaneDf, disastersInfo, name, locations\n",
    "\n",
    "def getFundingDf(funding, disastersInfo, name, locations):\n",
    "    fundingdf = pd.DataFrame(columns=['Year'] + locations, index=name)\n",
    "    for hurricane, locs in funding.items():\n",
    "        for loc, funds in locs.items():\n",
    "            fundingdf.at[hurricane, loc] = funds\n",
    "    for name, year in disastersInfo:\n",
    "        fundingdf.at[name, 'Year'] = year \n",
    "    fundingdf.rename(index=str, columns={\"- Any -\":\"Total\"}, inplace=True)\n",
    "    return fundingdf\n",
    "\n",
    "def getYoutubeDFs(disastersinfo):\n",
    "    return getAllDisasterDfs(disastersInfo)\n",
    "\n",
    "def getNOAAFunding(kwargs):\n",
    "    return getFundingDataFromNOAA(**kwargs)\n",
    "\n",
    "def processTweetDf(file):\n",
    "    df = pd.read_csv(file).drop_duplicates()\n",
    "    df['Total Exposure'] = df[['replies', 'retweets', 'likes']].sum(axis=1)\n",
    "    df.drop(['link', 'id', 'location'], axis=1, inplace=True)\n",
    "    df['date'] = df['date'].apply(pd.to_datetime)\n",
    "    prev = df['date'].max().year-1\n",
    "    b = date(prev, 7, 1)\n",
    "    mod_df = df[df['date']>=b]\n",
    "    return mod_df\n",
    "\n",
    "def processYoutubeDf(file):\n",
    "    df = pd.read_csv(file, index_col=0).reset_index().drop('index', axis=1)\n",
    "    df.drop(['Favourite Count', 'Video ID'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def getCompiledDf(hurricaneNames, fundingDf, hurricaneDf, tweets_by_hurricane, youtube_by_hurricane):\n",
    "    columns = ['Tweet Count', 'Tweet Exposure', 'Hashtags', 'Video Count', 'Video Exposure',\n",
    "                   'Youtube Tags']\n",
    "    num_rows = len(tweets_by_hurricane)\n",
    "    compiledDf = pd.DataFrame(columns=columns, index=hurricaneNames)\n",
    "    compiledDf = compiledDf.merge(fundingDf[['Total','Year']], how='outer', left_index=True, right_index=True)\n",
    "    compiledDf.rename(index=str, columns={\"Total\":\"Funding\"}, inplace=True)\n",
    "    compiledDf = compiledDf.merge(hurricaneDf[['Damages']], how='outer', left_index=True, right_index=True)\n",
    "    for hurricane in hurricaneNames:\n",
    "        if hurricane in tweets_by_hurricane:\n",
    "            tweets = tweets_by_hurricane[hurricane]\n",
    "            compiledDf.loc[hurricane, 'Tweet Count'] = len(tweets)\n",
    "            compiledDf.loc[hurricane, 'Tweet Exposure'] = sum(tweets['Total Exposure'])\n",
    "            allhashes = [val.replace('\\'', '') for res in tweets['hashtags'] if res != '[]' \\\n",
    "                                                 for val in res.replace('[', '').replace(']', '').split(', ')]\n",
    "            compiledDf.loc[hurricane, 'Hashtags'] = ','.join(allhashes)\n",
    "        vids = youtube_by_hurricane[hurricane]\n",
    "        compiledDf.loc[hurricane, 'Video Count'] = len(vids)\n",
    "        compiledDf.loc[hurricane, 'Video Exposure'] = sum(vids['Views'])\n",
    "        allTags = [val.replace('\\'', '') for res in vids['Tags'] if res != '[]' \\\n",
    "                                                 for val in res.replace('[', '').replace(']', '').split(', ')]\n",
    "        compiledDf.loc[hurricane, 'Youtube Tags'] = ','.join(allTags)\n",
    "    return compiledDf\n",
    "\n",
    "def plotFunding(df, xcol, title = 'Title', xaxis='x', yaxis='y', logx=True, logy=True):\n",
    "    temp = df.reset_index()\n",
    "    temp = temp[temp['Funding']>1]\n",
    "    if logy:\n",
    "        temp['Funding'] = np.log(temp['Funding'])\n",
    "    if logx:\n",
    "        temp[xcol] = np.log(temp[xcol])\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.lmplot(x=xcol, y='Funding', data=temp, hue='Name', fit_reg=False, legend=True)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xaxis)\n",
    "    plt.ylabel(yaxis)\n",
    "    return\n",
    "\n",
    "def plotTweetResponse(df, col='Total Exposure', label=''):\n",
    "    ax = plt.plot(df[['date', col]].groupby('date').sum(), label='')\n",
    "    plt.xlabel('Date')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylabel(col)\n",
    "    return\n",
    "    \n",
    "def plotTweetCounts(df, label=''):\n",
    "    ax = plt.plot(df.groupby('date').count(), label='')\n",
    "    plt.xlabel('Date')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylabel('Number of Tweets')\n",
    "    return\n",
    "    \n",
    "twitter_users_per_year = {2017: 328,\n",
    "                          2016: 319,\n",
    "                          2015: 305,\n",
    "                          2014: 288,\n",
    "                          2013: 241,\n",
    "                          2012: 185,\n",
    "                          2011: 117,\n",
    "                          2010: 54\n",
    "                            }\n",
    "\n",
    "### Specified For U.S: Data for 2014-2017 was publicly available. Data for 2005-2013 was extrapolated using a\n",
    "### quadratic fit on the found data: Formula: -0.817241 x^2 + 3300.01 x - 3.33117Ã—10^6\n",
    "youtube_users_per_year = {2017: 180.7,\n",
    "                          2016: 176.1,\n",
    "                          2015: 170.7,\n",
    "                          2014: 163.5,\n",
    "                          2013: 151.95,\n",
    "                          2012: 140.04,\n",
    "                          2011: 128.64,\n",
    "                          2010: 114.74,\n",
    "                          2009: 99.218,\n",
    "                          2008: 82.06,\n",
    "                          2007: 63.72,\n",
    "                          2006: 42.58,\n",
    "                          2005: 20.79\n",
    "                          }\n",
    "\n",
    "\n",
    "searchParamsFema = {'source' : 'https://www.fema.gov/disasters',\n",
    "                'location' : 'Florida',\n",
    "                'incidentType' : 'Hurricane',\n",
    "                'declarationType' : 'DR',\n",
    "                'startMonth' : 'January',\n",
    "                'startYear' : 2013,\n",
    "                'endMonth' : 'December',\n",
    "                'endYear' : 2018,\n",
    "                'disasterName' : 'Irma'}\n",
    "\n",
    "searchQueryNOAA = {\n",
    "    'eventType': '(Z) Storm Surge/Tide',\n",
    "    'beginDate_mm': '01',\n",
    "    'beginDate_dd': '01',\n",
    "    'beginDate_yyyy': '2013',\n",
    "    'endDate_mm': '12',\n",
    "    'endDate_dd': '30',\n",
    "    'endDate_yyyy': '2018',\n",
    "    'county': 'ALL',\n",
    "}\n",
    "\n",
    "# funding = getAllFunding(disastersInfo, searchParamsFema)\n",
    "# np.save(\"fundingDict.npy\", funding)\n",
    "\n",
    "### Collect The Fema Funds as a CSV File\n",
    "# funding = np.load('fundingDict.npy').item()\n",
    "# df = getFundingDf(funding, disastersInfo, hurricaneNames, locations)\n",
    "# df.to_csv('fundingRaised.csv')\n",
    "\n",
    "### Collect Youtube Stats as CSV file\n",
    "# YouTubeDictDfs = getYouTubeDfs(disastersInfo)\n",
    "# for idx, (key, value) in enumerate(YouTubeDictDfs.items()):\n",
    "#     value.to_csv('YouTube Dataframes/'+key+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDataFrames():\n",
    "    ### LOAD FUNDING DATAFRAME\n",
    "    fundingDf = pd.read_csv('fundingRaised.csv', index_col=0)\n",
    "    fundingDf.drop([col for col, val in fundingDf.sum().iteritems() if val == 0], axis=1, inplace=True)\n",
    "    \n",
    "    hurricaneDf, disastersInfo, hurricaneNames, locations = getDisasterInfo()\n",
    "    \n",
    "    ### LOAD ALL TWEET DATAFRAMES\n",
    "    tweets_by_hurricane = {}\n",
    "    for file in os.listdir(\"tweet_csv\"):\n",
    "        hurricane, form = file.split('.')\n",
    "        if form == 'csv':\n",
    "            tweets_by_hurricane[hurricane] = processTweetDf('tweet_csv/' + file)\n",
    "\n",
    "    ### LOAD ALL YOUTUBE DATAFRAMES\n",
    "    youtube_by_hurricane = {}\n",
    "    for file in os.listdir(\"Youtube_Dataframes\"):\n",
    "        hurricane = file.split('.')[0]\n",
    "        youtube_by_hurricane[hurricane] = processYoutubeDf('Youtube_Dataframes/' + file)\n",
    "\n",
    "    ### Group Relevant Columns Together Across Df's\n",
    "    df = getCompiledDf(hurricaneNames, fundingDf, hurricaneDf, tweets_by_hurricane, youtube_by_hurricane)\n",
    "    return fundingDf, df\n",
    "\n",
    "def adjustInflation(df):\n",
    "    ### Adjusting Funding and Damages for Inflation at an average of 1.89% per year [CITE THIS]\n",
    "    inflationAdjustedDf = df.copy()\n",
    "    def calcInflation(row, col='Funding'):\n",
    "        interest = np.power(1.0186, 2018-row['Year']+1)\n",
    "        return row[col] * interest\n",
    "    inflationAdjustedDf['Funding'] = inflationAdjustedDf.apply(lambda row: calcInflation(row, col='Funding'), axis=1)\n",
    "    inflationAdjustedDf['Damages'] = inflationAdjustedDf.apply(lambda row: calcInflation(row, col='Damages'), axis=1)\n",
    "    return inflationAdjustedDf\n",
    "\n",
    "def normalizeTweetsandYouTube(df):\n",
    "    normalizedDf = df.copy()\n",
    "    normalizedTweetDf = normalizedDf[normalizedDf.Year >= 2010].copy()\n",
    "    normalizedYouTubeDf = normalizedDf.copy()\n",
    "\n",
    "    normalizedTweetDf['Tweet Exposure'] = normalizedTweetDf.apply(lambda row: \\\n",
    "                                            row['Tweet Exposure']/(twitter_users_per_year[row.Year]-twitter_users_per_year[2010]+1), axis=1)\n",
    "\n",
    "    normalizedYouTubeDf['Video Exposure'] = normalizedYouTubeDf.apply(lambda row: \\\n",
    "                                                row['Video Exposure']/(youtube_users_per_year[row.Year]*1e6), axis=1)\n",
    "    return normalizedTweetDf, normalizedYouTubeDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
