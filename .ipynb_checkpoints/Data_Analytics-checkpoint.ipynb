{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Wiki_Hurricane_Scraper import getHurricaneData\n",
    "from YouTube_Analytics import *\n",
    "from FEMA_Scraper import *\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDisasterInfo():\n",
    "    hurricaneDf = getHurricaneData()\n",
    "    recentHurricanes = hurricaneDf[hurricaneDf['Season'] >= 2005]\n",
    "    name = recentHurricanes['Name'].tolist()\n",
    "    season = recentHurricanes['Season'].tolist()\n",
    "    locations = getAllLocations()\n",
    "    disastersInfo = list(zip(name, season))\n",
    "    return disastersInfo, name, locations\n",
    "\n",
    "def getFundingDf(funding, disastersInfo, name, locations):\n",
    "    fundingdf = pd.DataFrame(columns=['Year'] + locations, index=name)\n",
    "    for hurricane, locs in funding.items():\n",
    "        for loc, funds in locs.items():\n",
    "            fundingdf.at[hurricane, loc] = funds\n",
    "    for name, year in disastersInfo:\n",
    "        fundingdf.at[name, 'Year'] = year \n",
    "    return fundingdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "disastersInfo, name, locations = getDisasterInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "searchParams = {'source' : 'https://www.fema.gov/disasters',\n",
    "                'location' : 'Florida',\n",
    "                'incidentType' : 'Hurricane',\n",
    "                'declarationType' : 'DR',\n",
    "                'startMonth' : 'January',\n",
    "                'startYear' : 2013,\n",
    "                'endMonth' : 'December',\n",
    "                'endYear' : 2018,\n",
    "                'disasterName' : 'Irma'}\n",
    "\n",
    "# funding = getAllFunding(disastersInfo, searchParams)\n",
    "# np.save(\"fundingDict.npy\", funding)\n",
    "\n",
    "# funding = np.load('fundingDict.npy').item()\n",
    "# df = getFundingDf(funding, disastersInfo, name, locations)\n",
    "# df.to_csv('fundingRaised.csv')\n",
    "df = pd.read_csv('fundingRaised.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Error for query:Hurricane Katrina with token:CLYHEAA\n",
      "Token Error for query:Hurricane Harvey with token:CLYHEAA\n",
      "Token Error for query:Hurricane Maria with token:CLYHEAA\n",
      "Token Error for query:Hurricane Sandy with token:CLYHEAA\n",
      "Token Error for query:Hurricane Irma with token:CLYHEAA\n",
      "Token Error for query:Hurricane Ike with token:CLYHEAA\n",
      "Token Error for query:Hurricane Wilma with token:CLYHEAA\n",
      "Token Error for query:Hurricane Rita with token:CLYHEAA\n",
      "Token Error for query:Hurricane Matthew with token:CLYHEAA\n",
      "Token Error for query:Hurricane Irene with token:CLYHEAA\n",
      "Token Error for query:Hurricane Gustav with token:CLYHEAA\n",
      "Token Error for query:Hurricane Stan with token:CLYHEAA\n",
      "Token Error for query:Hurricane Karl with token:CLYHEAA\n",
      "Token Error for query:Hurricane Dennis with token:CLYHEAA\n",
      "Token Error for query:Hurricane Isaac with token:CLYHEAA\n",
      "Token Error for query:Hurricane Lee with token:CLYHEAA\n",
      "Token Error for query:Hurricane Dean with token:CLYHEAA\n",
      "Token Error for query:Hurricane Dolly with token:CLYHEAA\n",
      "Token Error for query:Hurricane Alex with token:CLYHEAA\n",
      "Token Error for query:Hurricane Ingrid with token:CLYHEAA\n",
      "Token Error for query:Hurricane Emily with token:CLYHEAA\n"
     ]
    }
   ],
   "source": [
    "# YouTubeDictDfs = getAllDisasterDfs(disastersInfo)\n",
    "# for idx, (key, value) in enumerate(YouTubeDictDfs.items()):\n",
    "#     value.to_csv('YouTube Dataframes/'+key+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
