{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube API v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "from YouTube_Scraper import youtube_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pltWidth, pltHeight = 20, 10\n",
    "plt.rcParams['figure.figsize'] = (pltWidth, pltHeight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getYouTubeVideoInfo(query=\"Hurricane\", maxResults=50, order=\"viewCount\",\n",
    "                        token=None, location=None, location_radius=None,\n",
    "                        publishedAfter=None, publishedBefore=None):\n",
    "\n",
    "    def mergeDictOfLists(dol1, dol2):\n",
    "        result = dict(dol1, **dol2)\n",
    "        result.update((k, dol1[k] + dol2[k]) for k in set(dol1).intersection(dol2))\n",
    "        return result\n",
    "    \n",
    "    results = None\n",
    "    nextToken = token\n",
    "    while True:\n",
    "        try:\n",
    "            test, nextToken = youtube_search(query, maxResults, order, nextToken, location,\n",
    "                                             location_radius, publishedAfter, publishedBefore)\n",
    "        except KeyError:\n",
    "            print('Token Error for query:{} with token:{}'.format(query, nextToken))\n",
    "            nextToken = None\n",
    "        time.sleep(0.1)\n",
    "        results = mergeDictOfLists(results, test) if results else test\n",
    "    \n",
    "        if nextToken is None:\n",
    "            break\n",
    "    \n",
    "    YouTubeVideoDf = pd.DataFrame(data=results)\n",
    "    YouTubeVideoDf = YouTubeVideoDf[['title', 'viewCount', 'channelTitle', 'commentCount', 'likeCount',\n",
    "                                     'dislikeCount', 'tags', 'favoriteCount', 'videoId']]\n",
    "    YouTubeVideoDf.columns = ['Title', 'Views', 'Channel', 'Comment Count', 'Likes', 'Dislikes', 'Tags',\n",
    "                              'Favourite Count', 'Video ID']\n",
    "    numericDtypes = ['Views', 'Comment Count', 'Likes', 'Dislikes', 'Favourite Count']\n",
    "    for i in numericDtypes:\n",
    "        YouTubeVideoDf[i] = YouTubeVideoDf[i].astype(int)\n",
    "    YouTubeVideoDf.sort_values(by=['Views'], ascending=False, inplace=True)\n",
    "    \n",
    "    return YouTubeVideoDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAllDisasterDfs(disasterInfo):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        disasterInfo (list of tuples): Each tuple is of the form (Disaster_Name, Year)\n",
    "    Returns:\n",
    "        YouTubeDfsList (list of Dataframes): Each dataframe corresponds to the videos of the \n",
    "                respective diaster and year from diasterInfo\n",
    "    \"\"\"\n",
    "    disasterDf = {}\n",
    "    \n",
    "    for disasterName, year in disasterInfo:\n",
    "        query = 'Hurricane ' + disasterName\n",
    "        publishedAfter = str(year) + '-01-01T00:00:00Z'\n",
    "        \n",
    "        videoDf = getYouTubeVideoInfo(query=query, publishedAfter=publishedAfter)\n",
    "        disasterDf[disasterName] = videoDf\n",
    "    \n",
    "    return disasterDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeSpamChannels(videoDfs, channelList=['VEVO'], existsPartly=True):\n",
    "    for channel in channelList:\n",
    "        if existsPartly:\n",
    "            videoDfs = videoDfs[~videoDfs.Channel.str.contains(channel)]\n",
    "        else:\n",
    "            videoDfs = videoDfs[~(videoDfs.Channel == channel)]\n",
    "    return videoDfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotVideoDf(df1):\n",
    "    df1 = df1.sort_values(ascending=False, by='Views')\n",
    "    plt.bar(range(df1.shape[0]), df1['Views'])\n",
    "    plt.xticks(range(df1.shape[0]), df1['Title'], rotation=90)\n",
    "    plt.ylabel('Views in 10 millions')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
