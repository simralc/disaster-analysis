{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis for Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk.classify.util\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.metrics import precision, recall\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.classify import SklearnClassifier\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all stopwords from the Wordcloud and NLTK package.\n",
    "twitter_stopwords = sorted(list(set(stopwords.words('english')).union(set(STOPWORDS))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_correct_word(word):\n",
    "    '''\n",
    "    Check if the word falls into any of the unwanted categories for sentiment classification\n",
    "    \n",
    "    Args:\n",
    "        word (string): A single word of the tweet text\n",
    "        \n",
    "    Returns:\n",
    "        (boolean): False if word in unwanted categories, otherwise True\n",
    "    '''\n",
    "    if word in twitter_stopwords:\n",
    "        return False\n",
    "    if 'http' in word or '://' in word or word.startswith('@') or word.startswith('#') or word == 'RT':\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def process_text(tweet_text):\n",
    "    '''\n",
    "    Split tweet text into a list of lower case words and remove unwanted words.\n",
    "    \n",
    "    Args:\n",
    "        tweet_text (string): A single row of tweets training dataframe.\n",
    "        \n",
    "    Returns:\n",
    "        tweet_words (list): A list of required words after splitting of tweet_text\n",
    "    '''\n",
    "    tweet_words = [word for word in tweet_text.lower().split(' ') if is_correct_word(word)]\n",
    "    return tweet_words\n",
    "\n",
    "def get_train_df():\n",
    "    '''\n",
    "    Compile all datasets of tweet data for sentiment classification.\n",
    "    \n",
    "    Args:\n",
    "        None\n",
    "        \n",
    "    Returns:\n",
    "        train_df (pd.DataFrame): A pandas DataFrame with tweet text and corresponding sentiment label\n",
    "    '''\n",
    "    columns_dataset1 = ['Index', 'sentiment', 'Source', 'text']\n",
    "    train_set_1 = pd.read_csv('../Tweet Sentiment Datasets/dataset1.csv',\n",
    "                              error_bad_lines=False, warn_bad_lines=False)\n",
    "\n",
    "    columns_dataset2 = ['sentiment', 'tweetId', 'Date&Time', 'query', 'user', 'text']\n",
    "    train_set_2 = pd.read_csv('../Tweet Sentiment Datasets/dataset2.csv', encoding = \"latin1\",\n",
    "                              error_bad_lines=False, warn_bad_lines=False, header=None)\n",
    "\n",
    "    columns_dataset3 = ['sentiment', 'text']\n",
    "    train_set_3 = pd.read_csv('../Tweet Sentiment Datasets/dataset3.tsv', sep='\\t',\n",
    "                              error_bad_lines=False, warn_bad_lines=False)\n",
    "    \n",
    "    train_set_1.columns = columns_dataset1\n",
    "    train_set_2.columns = columns_dataset2\n",
    "    train_set_3.columns = columns_dataset3\n",
    "    \n",
    "    train_set_2['sentiment'] = train_set_2.sentiment.apply(lambda x: 1 if x==4 else x)\n",
    "    \n",
    "    train_df = train_set_1[['sentiment', 'text']]\n",
    "    train_df = pd.concat([train_df, train_set_2[['sentiment', 'text']]], ignore_index=True)\n",
    "    train_df = pd.concat([train_df, train_set_3[['sentiment', 'text']]], ignore_index=True)\n",
    "    \n",
    "    train_df['sentiment'] = train_df.sentiment.apply(lambda x: int(x))\n",
    "    train_df['text'] = train_df.text.apply(lambda tweet_text: process_text(tweet_text))\n",
    "    \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframes(train_df, test_size=0.1):\n",
    "    '''\n",
    "    Split the DataFrame into train and test for each of the labels (positive & negative)\n",
    "    \n",
    "    Args:\n",
    "        train_df (pd.DataFrame): A pandas DataFrame with tweet text and corresponding sentiment label\n",
    "        test_size (float): The percentage of rows to be considered for test size\n",
    "        \n",
    "    Returns:\n",
    "        train_pos (pd.DataFrame): A split of train_df containing positive train tweets\n",
    "        train_neg (pd.DataFrame): A split of train_df containing negative train tweets\n",
    "        test_pos (pd.DataFrame): A split of train_df containing positive test tweets\n",
    "        test_neg (pd.DataFrame): A split of train_df containing negative test tweets\n",
    "    '''\n",
    "    train, test = train_test_split(train_df, test_size=test_size)\n",
    "    \n",
    "    train_pos = train[train['sentiment'] == 1]['text']\n",
    "    train_neg = train[train['sentiment'] == 0]['text']\n",
    "    test_pos = test[test['sentiment'] == 1]['text']\n",
    "    test_neg = test[test['sentiment'] == 0]['text']\n",
    "    \n",
    "    return train_pos, train_neg, test_pos, test_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1594119,) (0,)\n"
     ]
    }
   ],
   "source": [
    "# Get the training DataFrame for sentiment classification.\n",
    "train_df = get_train_df()\n",
    "train_pos, train_neg, test_pos, test_neg = split_dataframes(train_df, test_size=0)\n",
    "\n",
    "print(train_pos.shape, test_pos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_feats(text_dataframe, feat='pos'):\n",
    "    '''\n",
    "    Convert the DataFrame into word feature tuples for NLTKs NaiveBayes Classifier.\n",
    "    Args:\n",
    "        text_dataframe (pd.DataFrame): A pandas DataFrame with tweet text and corresponding sentiment label\n",
    "    Returns:\n",
    "        (list): A list of tuples (word, feature) for each word in text of text_dataframe\n",
    "    '''\n",
    "    def word_feats(words):\n",
    "        return dict([(word, True) for word in words])\n",
    "    \n",
    "    return [(word_feats(word_list), feat) for word_list in text_dataframe]\n",
    "\n",
    "\n",
    "def train_naive_bayes_classifier(train_pos, train_neg):\n",
    "    '''\n",
    "    Train a NaiveBayes Classifier for the negative and positive sentiment texts given.\n",
    "    Args:\n",
    "        train_pos (pd.DataFrame): A split of train_df containing positive train tweets\n",
    "        train_neg (pd.DataFrame): A split of train_df containing negative train tweets\n",
    "    Returns:\n",
    "        (NaiveBayesClassifier): A classifier that is trained on the training dataframe features\n",
    "    '''\n",
    "    train_neg_feats = get_text_feats(train_neg, 'neg')\n",
    "    train_pos_feats = get_text_feats(train_pos)\n",
    " \n",
    "    trainfeats = train_neg_feats + train_pos_feats\n",
    "    \n",
    "    return NaiveBayesClassifier.train(trainfeats)\n",
    "\n",
    "def test_naive_bayes_classifier(classifier, test_neg, test_pos):\n",
    "    '''\n",
    "    Test on a pretrianed NaiveBayes Classifier for the negative and positive sentiment texts given.\n",
    "    Args:\n",
    "        classifier (NaiveBayesClassifier): A classifier that is trained on the training set\n",
    "        train_pos (pd.DataFrame): A split of train_df containing positive train tweets\n",
    "        train_neg (pd.DataFrame): A split of train_df containing negative train tweets\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    test_neg_feats = get_text_feats(test_neg, 'neg')\n",
    "    test_pos_feats = get_text_feats(test_pos)\n",
    "    \n",
    "    testfeats = test_neg_feats + test_pos_feats\n",
    "    \n",
    "    refsets = collections.defaultdict(set)\n",
    "    testsets = collections.defaultdict(set)\n",
    "    \n",
    "    for i, (feats, label) in enumerate(testfeats):\n",
    "        refsets[label].add(i)\n",
    "        observed = classifier.classify(feats)\n",
    "        testsets[observed].add(i)\n",
    " \n",
    "    print('accuracy:', nltk.classify.util.accuracy(classifier, testfeats))\n",
    "    print('pos precision:', precision(refsets['pos'], testsets['pos']))\n",
    "    print('pos recall:', recall(refsets['pos'], testsets['pos']))\n",
    "    print('neg precision:', precision(refsets['neg'], testsets['neg']))\n",
    "    print('neg recall:', recall(refsets['neg'], testsets['neg']))\n",
    "    classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = train_naive_bayes_classifier(train_neg, train_pos)\n",
    "\n",
    "# Test the classifier over the test DataFrames if it exists\n",
    "if test_pos.shape[0]:\n",
    "    test_naive_bayes_classifier(classifier, test_neg, test_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.15571416866711935\n",
      "pos precision: 0.18213722754486847\n",
      "pos recall: 0.19686673328653634\n",
      "neg precision: 0.12458384416198916\n",
      "neg recall: 0.11449155151720801\n",
      "Most Informative Features\n",
      "                     447 = True              pos : neg    =     88.8 : 1.0\n",
      "                 me..its = True              neg : pos    =     76.0 : 1.0\n",
      "                  farrah = True              pos : neg    =     61.4 : 1.0\n",
      "                    sadd = True              pos : neg    =     60.7 : 1.0\n",
      "                 *cries* = True              pos : neg    =     56.2 : 1.0\n",
      "                 saddens = True              pos : neg    =     54.8 : 1.0\n",
      "                     228 = True              pos : neg    =     53.9 : 1.0\n",
      "                 bummed. = True              pos : neg    =     50.9 : 1.0\n",
      "                 sadface = True              pos : neg    =     50.7 : 1.0\n",
      "               pleasure! = True              neg : pos    =     50.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "test_naive_bayes_classifier(classifier, train_neg, train_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_classify(text):\n",
    "    feats = get_text_feats([text])\n",
    "    return 1 if classifier.classify(feats[0][0]) == 'pos' else 0\n",
    "    \n",
    "def processTweetDf(file):\n",
    "    df = pd.read_csv(file)\n",
    "    df['Total Exposure'] = df[['replies', 'retweets', 'likes']].sum(axis=1)\n",
    "    df.drop(['link', 'id', 'location'], axis=1, inplace=True)\n",
    "    \n",
    "    df.rename(index=str, columns={\"tweet\": \"text\"}, inplace=True)\n",
    "    df['text'] = df.text.apply(lambda tweet_text: process_text(tweet_text))\n",
    "    \n",
    "    df['sentiment'] = df.text.apply(lambda x: my_classify(x))\n",
    "    return df\n",
    "\n",
    "tweets_by_hurricane = {}\n",
    "for file in os.listdir(\"tweet_csv\"):\n",
    "    hurricane = file.split('.')[0]\n",
    "    tweets_by_hurricane[hurricane] = processTweetDf('tweet_csv/' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats_tweet_sentiments(tweet_df_dict):\n",
    "    hurricane_tweet_stats = {}\n",
    "    hurricane_stat_ratios = {}\n",
    "    \n",
    "    for hurricane, df in tweet_df_dict.items():\n",
    "        value_counts = df.sentiment.value_counts()\n",
    "        hurricane_tweet_stats[hurricane] = value_counts\n",
    "        hurricane_stat_ratios[hurricane] = value_counts[1]/value_counts[0]\n",
    "        \n",
    "    return hurricane_tweet_stats, hurricane_stat_ratios\n",
    "\n",
    "hurricane_tweet_stats, hurricane_stat_ratios = get_stats_tweet_sentiments(tweets_by_hurricane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hurricane</th>\n",
       "      <th>Tweet Sentiment Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Harvey</td>\n",
       "      <td>1.659696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maria</td>\n",
       "      <td>1.867185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Irma</td>\n",
       "      <td>1.070403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Matthew</td>\n",
       "      <td>1.398477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Karl</td>\n",
       "      <td>3.706452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alex</td>\n",
       "      <td>1.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dolly</td>\n",
       "      <td>2.020761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dean</td>\n",
       "      <td>1.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Gustav</td>\n",
       "      <td>6.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ike</td>\n",
       "      <td>2.563025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Ingrid</td>\n",
       "      <td>1.434783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Irene</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Isaac</td>\n",
       "      <td>1.629371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Lee</td>\n",
       "      <td>2.449275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sandy</td>\n",
       "      <td>1.506655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Hurricane  Tweet Sentiment Ratio\n",
       "0     Harvey               1.659696\n",
       "1      Maria               1.867185\n",
       "2       Irma               1.070403\n",
       "3    Matthew               1.398477\n",
       "4       Karl               3.706452\n",
       "5       Alex               1.071429\n",
       "6      Dolly               2.020761\n",
       "7       Dean               1.250000\n",
       "8     Gustav               6.307692\n",
       "9        Ike               2.563025\n",
       "10    Ingrid               1.434783\n",
       "11     Irene               1.500000\n",
       "12     Isaac               1.629371\n",
       "13       Lee               2.449275\n",
       "14     Sandy               1.506655"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = pd.Series(list(hurricane_stat_ratios.keys()), name=\"Hurricane\")\n",
    "s2 = pd.Series(list(hurricane_stat_ratios.values()), name=\"Tweet Sentiment Ratio\")\n",
    "\n",
    "pd.concat([s1, s2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud_draw(data, color='black'):\n",
    "    words = ' '.join(data)\n",
    "    cleaned_word = \" \".join([word for word in words.split()\n",
    "                            if 'http' not in word\n",
    "                                and not word.startswith('@')\n",
    "                                and not word.startswith('#')\n",
    "                                and word != 'RT'\n",
    "                            ])\n",
    "    wordcloud = WordCloud(stopwords=twitter_stopwords,\n",
    "                      background_color=color,\n",
    "                      width=2500,\n",
    "                      height=2000\n",
    "                     ).generate(cleaned_word)\n",
    "    plt.figure(1, figsize=(13, 13))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# print(\"Positive words\")\n",
    "# wordcloud_draw(train_pos, 'white')\n",
    "# print(\"Negative words\")\n",
    "# wordcloud_draw(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(train):\n",
    "    tweets = []\n",
    "    stopwords_set = set(stopwords.words(\"english\"))\n",
    "\n",
    "    for row in train.itertuples():\n",
    "        words_filtered = [e.lower() for e in row.text if len(e) >= 3]\n",
    "        words_cleaned = [word for word in words_filtered\n",
    "            if 'http' not in word\n",
    "            and '://' not in word\n",
    "            and not word.startswith('@')\n",
    "            and not word.startswith('#')\n",
    "            and word not in stopwords_set\n",
    "            and word != 'RT']\n",
    "        tweets.append((words_cleaned, row.sentiment))\n",
    "    \n",
    "    return tweets\n",
    "\n",
    "# Extracting word features\n",
    "def get_words_in_tweets(tweets):\n",
    "    all = []\n",
    "    for (words, sentiment) in tweets:\n",
    "        all.extend(words)\n",
    "    return all\n",
    "\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    features = wordlist.keys()\n",
    "    return features\n",
    "\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in w_features:\n",
    "        features['containts(%s)' % word] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "\n",
    "# train, test = train_test_split(train_df, test_size=0.1)\n",
    "# train_ = train.iloc[:100]\n",
    "# test_ = test.iloc[:50]\n",
    "# tweets = clean_tweets(train_)\n",
    "# test_tweets = clean_tweets(test_)\n",
    "# w_features = get_word_features(get_words_in_tweets(tweets))\n",
    "# wordcloud_draw(w_features)\n",
    "# test_w_features = get_word_features(get_words_in_tweets(test_tweets))\n",
    "# wordcloud_draw(test_w_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = nltk.classify.apply_features(extract_features, tweets)\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = nltk.classify.apply_features(extract_features, test_tweets)\n",
    "print('accuracy:', nltk.classify.util.accuracy(classifier, test_set))\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_cnt = 0\n",
    "pos_cnt = 0\n",
    "for obj in test_neg: \n",
    "    res =  classifier.classify(extract_features(obj.split()))\n",
    "    if(res == 'Negative'): \n",
    "        neg_cnt = neg_cnt + 1\n",
    "for obj in test_pos: \n",
    "    res =  classifier.classify(extract_features(obj.split()))\n",
    "    if(res == 'Positive'): \n",
    "        pos_cnt = pos_cnt + 1\n",
    "        \n",
    "print('[Negative]: %s/%s '  % (len(test_neg),neg_cnt))        \n",
    "print('[Positive]: %s/%s '  % (len(test_pos),pos_cnt))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import collections, itertools\n",
    "import nltk.classify.util, nltk.metrics\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews, stopwords\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "\n",
    "def evaluate_classifier(featx):\n",
    "    negids = movie_reviews.fileids('neg')\n",
    "    posids = movie_reviews.fileids('pos')\n",
    "    \n",
    "    negfeats = [(featx(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "    posfeats = [(featx(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    "    \n",
    "    negcutoff = len(negfeats)*3//4\n",
    "    poscutoff = len(posfeats)*3//4\n",
    "    \n",
    "    trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "    testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "    \n",
    "    classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "    refsets = collections.defaultdict(set)\n",
    "    testsets = collections.defaultdict(set)\n",
    "    \n",
    "    for i, (feats, label) in enumerate(testfeats):\n",
    "            refsets[label].add(i)\n",
    "            observed = classifier.classify(feats)\n",
    "            testsets[observed].add(i)\n",
    "    \n",
    "    print('accuracy:', nltk.classify.util.accuracy(classifier, testfeats))\n",
    "    print('pos precision:', precision(refsets['pos'], testsets['pos']))\n",
    "    print('pos recall:', recall(refsets['pos'], testsets['pos']))\n",
    "    print('neg precision:', precision(refsets['neg'], testsets['neg']))\n",
    "    print('neg recall:', recall(refsets['neg'], testsets['neg']))\n",
    "    \n",
    "    classifier.show_most_informative_features()\n",
    "\n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "# print('evaluating single word features')\n",
    "# evaluate_classifier(word_feats)\n",
    "\n",
    "# word_fd = FreqDist()\n",
    "# label_word_fd = ConditionalFreqDist()\n",
    "\n",
    "# for word in movie_reviews.words(categories=['pos']):\n",
    "#     word_fd.update(word.lower())\n",
    "#     label_word_fd['pos'].update(word.lower())\n",
    "\n",
    "# for word in movie_reviews.words(categories=['neg']):\n",
    "#     word_fd.update(word.lower())\n",
    "#     label_word_fd['neg'].update(word.lower())\n",
    "\n",
    "# n_ii = label_word_fd[label][word]\n",
    "# n_ix = word_fd[word]\n",
    "# n_xi = label_word_fd[label].N()\n",
    "# n_xx = label_word_fd.N()\n",
    "\n",
    "pos_word_count = label_word_fd['pos'].N()\n",
    "neg_word_count = label_word_fd['neg'].N()\n",
    "total_word_count = pos_word_count + neg_word_count\n",
    "\n",
    "word_scores = {}\n",
    "\n",
    "for word, freq in word_fd.items():\n",
    "    pos_score = BigramAssocMeasures.chi_sq(label_word_fd['pos'][word],\n",
    "        (freq, pos_word_count), total_word_count)\n",
    "    neg_score = BigramAssocMeasures.chi_sq(label_word_fd['neg'][word],\n",
    "        (freq, neg_word_count), total_word_count)\n",
    "    word_scores[word] = pos_score + neg_score\n",
    "\n",
    "best = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)[:10000]\n",
    "bestwords = set([w for w, s in best])\n",
    "\n",
    "def best_word_feats(words):\n",
    "    return dict([(word, True) for word in words if word in bestwords])\n",
    "\n",
    "print('evaluating best word features')\n",
    "evaluate_classifier(best_word_feats)\n",
    "\n",
    "def best_bigram_word_feats(words, score_fn=BigramAssocMeasures.chi_sq, n=200):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    bigrams = bigram_finder.nbest(score_fn, n)\n",
    "    d = dict([(bigram, True) for bigram in bigrams])\n",
    "    d.update(best_word_feats(words))\n",
    "    return d\n",
    "\n",
    "print('evaluating best words + bigram chi_sq word features')\n",
    "evaluate_classifier(best_bigram_word_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
