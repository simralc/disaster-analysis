{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis for Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk.classify.util\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.metrics import precision, recall\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.classify import SklearnClassifier\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_df():\n",
    "    columns_dataset1 = ['Index', 'sentiment', 'Source', 'text']\n",
    "    train_set_1 = pd.read_csv('../Tweet Sentiment Datasets/dataset1.csv',\n",
    "                              error_bad_lines=False, warn_bad_lines=False)\n",
    "\n",
    "    columns_dataset2 = ['sentiment', 'tweetId', 'Date&Time', 'query', 'user', 'text']\n",
    "    train_set_2 = pd.read_csv('../Tweet Sentiment Datasets/dataset2.csv', encoding = \"latin1\",\n",
    "                              error_bad_lines=False, warn_bad_lines=False, header=None)\n",
    "\n",
    "    columns_dataset3 = ['sentiment', 'text']\n",
    "    train_set_3 = pd.read_csv('../Tweet Sentiment Datasets/dataset3.tsv', sep='\\t',\n",
    "                              error_bad_lines=False, warn_bad_lines=False)\n",
    "    \n",
    "    train_set_1.columns = columns_dataset1\n",
    "    train_set_2.columns = columns_dataset2\n",
    "    train_set_3.columns = columns_dataset3\n",
    "    \n",
    "    train_set_2['sentiment'] = train_set_2.sentiment.apply(lambda x: 1 if x==4 else x)\n",
    "    \n",
    "    train_df = train_set_1[['sentiment', 'text']]\n",
    "    train_df = pd.concat([train_df, train_set_2[['sentiment', 'text']]], ignore_index=True)\n",
    "    train_df = pd.concat([train_df, train_set_3[['sentiment', 'text']]], ignore_index=True)\n",
    "    \n",
    "    train_df['sentiment'] = train_df.sentiment.apply(lambda x: int(x))\n",
    "    train_df['text'] = train_df.text.apply(lambda x: x.split(' '))\n",
    "    \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2866976, 2) (318553, 2)\n"
     ]
    }
   ],
   "source": [
    "train_df = get_train_df()\n",
    "train, test = train_test_split(train_df, test_size=0.1)\n",
    "\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = train[train['sentiment'] == 1]['text']\n",
    "train_neg = train[train['sentiment'] == 0]['text']\n",
    "test_pos = test[test['sentiment'] == 1]['text']\n",
    "test_neg = test[test['sentiment'] == 0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "def evaluate_classifier(featx, train_neg, train_pos, test_neg, test_pos):\n",
    "    train_neg_feats = [(featx(word_list), 'neg') for word_list in train_neg]\n",
    "    train_pos_feats = [(featx(word_list), 'pos') for word_list in train_pos]\n",
    "    \n",
    "    test_neg_feats = [(featx(word_list), 'neg') for word_list in test_neg]\n",
    "    test_pos_feats = [(featx(word_list), 'pos') for word_list in test_pos]\n",
    " \n",
    "    trainfeats = train_neg_feats + train_pos_feats\n",
    "    testfeats = test_neg_feats + train_pos_feats\n",
    " \n",
    "    classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "    refsets = collections.defaultdict(set)\n",
    "    testsets = collections.defaultdict(set)\n",
    "    \n",
    "    for i, (feats, label) in enumerate(testfeats):\n",
    "            refsets[label].add(i)\n",
    "            observed = classifier.classify(feats)\n",
    "            testsets[observed].add(i)\n",
    " \n",
    "    print('accuracy:', nltk.classify.util.accuracy(classifier, testfeats))\n",
    "    print('pos precision:', precision(refsets['pos'], testsets['pos']))\n",
    "    print('pos recall:', recall(refsets['pos'], testsets['pos']))\n",
    "    print('neg precision:', precision(refsets['neg'], testsets['neg']))\n",
    "    print('neg recall:', recall(refsets['neg'], testsets['neg']))\n",
    "    classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8283657367683067\n",
      "pos precision: 0.9038404858355096\n",
      "pos recall: 0.7352573824605227\n",
      "neg precision: 0.7765431488684791\n",
      "neg recall: 0.9216381749274557\n",
      "Most Informative Features\n",
      "                 me..its = True              pos : neg    =    154.1 : 1.0\n",
      "                 bummed. = True              neg : pos    =    131.9 : 1.0\n",
      "                   Died! = True              neg : pos    =     95.2 : 1.0\n",
      "             @Banksyart2 = True              pos : neg    =     89.5 : 1.0\n",
      "                Fuzzball = True              pos : neg    =     74.2 : 1.0\n",
      "                     228 = True              neg : pos    =     65.1 : 1.0\n",
      "                     447 = True              neg : pos    =     63.5 : 1.0\n",
      "                  Farrah = True              neg : pos    =     62.4 : 1.0\n",
      "                 McMahon = True              neg : pos    =     62.3 : 1.0\n",
      "                  sad!!! = True              neg : pos    =     61.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "evaluate_classifier(word_feats, train_neg, train_pos, test_neg, test_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud_draw(data, color='black'):\n",
    "    words = ' '.join(data)\n",
    "    cleaned_word = \" \".join([word for word in words.split()\n",
    "                            if 'http' not in word\n",
    "                                and not word.startswith('@')\n",
    "                                and not word.startswith('#')\n",
    "                                and word != 'RT'\n",
    "                            ])\n",
    "    wordcloud = WordCloud(stopwords=STOPWORDS,\n",
    "                      background_color=color,\n",
    "                      width=2500,\n",
    "                      height=2000\n",
    "                     ).generate(cleaned_word)\n",
    "    plt.figure(1,figsize=(13, 13))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "print(\"Positive words\")\n",
    "wordcloud_draw(train_pos, 'white')\n",
    "print(\"Negative words\")\n",
    "wordcloud_draw(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(train):\n",
    "    tweets = []\n",
    "    stopwords_set = set(stopwords.words(\"english\"))\n",
    "\n",
    "    for row in train.itertuples():\n",
    "        words_filtered = [e.lower() for e in row.text.split() if len(e) >= 3]\n",
    "        words_cleaned = [word for word in words_filtered\n",
    "            if 'http' not in word\n",
    "            and '://' not in word\n",
    "            and not word.startswith('@')\n",
    "            and not word.startswith('#')\n",
    "            and word not in stopwords_set\n",
    "            and word != 'RT']\n",
    "        tweets.append((words_cleaned, row.sentiment))\n",
    "    \n",
    "    return tweets\n",
    "\n",
    "# Extracting word features\n",
    "def get_words_in_tweets(tweets):\n",
    "    all = []\n",
    "    for (words, sentiment) in tweets:\n",
    "        all.extend(words)\n",
    "    return all\n",
    "\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    features = wordlist.keys()\n",
    "    return features\n",
    "\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in w_features:\n",
    "        features['containts(%s)' % word] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "# tweets = clean_tweets(train_)\n",
    "# w_features = get_word_features(get_words_in_tweets(tweets))\n",
    "# wordcloud_draw(w_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = nltk.classify.apply_features(extract_features, tweets)\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy:', nltk.classify.util.accuracy(classifier, test))\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_cnt = 0\n",
    "pos_cnt = 0\n",
    "for obj in test_neg: \n",
    "    res =  classifier.classify(extract_features(obj.split()))\n",
    "    if(res == 'Negative'): \n",
    "        neg_cnt = neg_cnt + 1\n",
    "for obj in test_pos: \n",
    "    res =  classifier.classify(extract_features(obj.split()))\n",
    "    if(res == 'Positive'): \n",
    "        pos_cnt = pos_cnt + 1\n",
    "        \n",
    "print('[Negative]: %s/%s '  % (len(test_neg),neg_cnt))        \n",
    "print('[Positive]: %s/%s '  % (len(test_pos),pos_cnt))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
