{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Wiki_Hurricane_Scraper import getHurricaneData\n",
    "from YouTube_Analytics import *\n",
    "from FEMA_Scraper import *\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDisasterInfo():\n",
    "    hurricaneDf = getHurricaneData()\n",
    "    recentHurricanes = hurricaneDf[hurricaneDf['Season'] >= 2005]\n",
    "    name = recentHurricanes['Name'].tolist()\n",
    "    season = recentHurricanes['Season'].tolist()\n",
    "    locations = getAllLocations()\n",
    "    disastersInfo = list(zip(name, season))\n",
    "    return disastersInfo, name, locations\n",
    "\n",
    "def getFundingDf(funding, disastersInfo, name, locations):\n",
    "    fundingdf = pd.DataFrame(columns=['Year'] + locations, index=name)\n",
    "    for hurricane, locs in funding.items():\n",
    "        for loc, funds in locs.items():\n",
    "            fundingdf.at[hurricane, loc] = funds\n",
    "    for name, year in disastersInfo:\n",
    "        fundingdf.at[name, 'Year'] = year \n",
    "    return fundingdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "disastersInfo, name, locations = getDisasterInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchParams = {'source' : 'https://www.fema.gov/disasters',\n",
    "                'location' : 'Florida',\n",
    "                'incidentType' : 'Hurricane',\n",
    "                'declarationType' : 'DR',\n",
    "                'startMonth' : 'January',\n",
    "                'startYear' : 2013,\n",
    "                'endMonth' : 'December',\n",
    "                'endYear' : 2018,\n",
    "                'disasterName' : 'Irma'}\n",
    "\n",
    "# funding = getAllFunding(disastersInfo, searchParams)\n",
    "# np.save(\"fundingDict.npy\", funding)\n",
    "\n",
    "# funding = np.load('fundingDict.npy').item()\n",
    "# df = getFundingDf(funding, disastersInfo, name, locations)\n",
    "# df.to_csv('fundingRaised.csv')\n",
    "# df = pd.read_csv('fundingRaised.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Error for query:Hurricane Katrina with token:CLYHEAA\n",
      "CKYEEAA Hurricane Harvey 2017-01-01T00:00:00Z 2019-01-01T00:00:00Z\n",
      "[]\n",
      "Token Error for query:Hurricane Harvey with token:CLYHEAA\n",
      "Token Error for query:Hurricane Maria with token:CLYHEAA\n",
      "Token Error for query:Hurricane Sandy with token:CLYHEAA\n",
      "Token Error for query:Hurricane Irma with token:CLYHEAA\n",
      "Token Error for query:Hurricane Ike with token:CLYHEAA\n",
      "Token Error for query:Hurricane Wilma with token:CLYHEAA\n",
      "Token Error for query:Hurricane Rita with token:CLYHEAA\n",
      "Token Error for query:Hurricane Matthew with token:CLYHEAA\n",
      "Token Error for query:Hurricane Irene with token:CLYHEAA\n",
      "Token Error for query:Hurricane Gustav with token:CLYHEAA\n",
      "Token Error for query:Hurricane Stan with token:CJYBEAA\n",
      "Token Error for query:Hurricane Karl with token:CLYHEAA\n",
      "Token Error for query:Hurricane Dennis with token:CPQDEAA\n",
      "Token Error for query:Hurricane Isaac with token:CLYHEAA\n",
      "Token Error for query:Hurricane Lee with token:CLYHEAA\n",
      "Token Error for query:Hurricane Dean with token:CLYHEAA\n",
      "Token Error for query:Hurricane Dolly with token:CLYHEAA\n",
      "Token Error for query:Hurricane Alex with token:CLYHEAA\n",
      "Token Error for query:Hurricane Ingrid with token:CLYHEAA\n",
      "Token Error for query:Hurricane Emily with token:CMIDEAA\n"
     ]
    }
   ],
   "source": [
    "YouTubeDictDfs = getAllDisasterDfs(disastersInfo)\n",
    "for idx, (key, value) in enumerate(YouTubeDictDfs.items()):\n",
    "    value.to_csv('YouTube Dataframes/'+key+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Katrina (358, 9)\n",
      "Harvey (502, 9)\n",
      "Maria (479, 9)\n",
      "Sandy (541, 9)\n",
      "Irma (508, 9)\n",
      "Ike (399, 9)\n",
      "Wilma (313, 9)\n",
      "Rita (287, 9)\n",
      "Matthew (509, 9)\n",
      "Irene (462, 9)\n",
      "Gustav (236, 9)\n",
      "Stan (124, 9)\n",
      "Karl (217, 9)\n",
      "Dennis (260, 9)\n",
      "Isaac (241, 9)\n",
      "Lee (148, 9)\n",
      "Dean (270, 9)\n",
      "Dolly (128, 9)\n",
      "Alex (152, 9)\n",
      "Ingrid (122, 9)\n",
      "Emily (219, 9)\n"
     ]
    }
   ],
   "source": [
    "for idx, (key, value) in enumerate(YouTubeDictDfs.items()):\n",
    "    print(key, value[value.Views > 1000].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
